# CONTEXT WINDOW OPTIMIZATION PLAN
# ==================================
# Optimizing token usage within Mastra framework constraints
# Based on insights from Anthropic and Cline articles

## CURRENT STATE ANALYSIS
## -----------------------
- Memory: SQLite persistence with basic schema (plan/status)
- Token counting: Rough approximation (chars/4)
- Compression: SummarizeTail processor (disabled)
- Tools: 25+ frontend tools with full document reads
- Context budget: 160k input / 40k output tokens
- No intelligent context selection or pruning

## OPTIMIZATION PRINCIPLES
## ------------------------
1. **High-Impact Workflows** - Consolidate related operations
2. **Token Efficiency** - Minimize redundant data transfer
3. **Smart Context Selection** - Pull only what's needed
4. **Narrative Integrity** - Preserve task continuity
5. **Progressive Enhancement** - Start simple, add intelligence

## PHASE 1: TOOL CONSOLIDATION & EFFICIENCY
## -----------------------------------------

### 1.1 Merge Redundant Tools
- [ ] Combine read_document + read_unified_document → smart_read
- [ ] Merge search variants → unified_search with options
- [ ] Consolidate edit operations → batch_edit for multiple changes
- [ ] Combine table operations → table_manager

### 1.2 Add Response Format Control
- [ ] Add `verbosity` enum to all tools: minimal|standard|detailed
- [ ] Implement `fields` parameter to select specific data
- [ ] Add `limit` parameter for list operations
- [ ] Default to minimal responses, expand on request

### 1.3 Implement Smart Defaults
- [ ] Auto-detect when full document read isn't needed
- [ ] Use paragraph IDs for targeted operations
- [ ] Cache frequently accessed sections
- [ ] Return only changed content after edits

**Expected savings: 30-40% token reduction**

## PHASE 2: INTELLIGENT CONTEXT MANAGEMENT
## ----------------------------------------

### 2.1 Document Structure Indexing
- [ ] Create document_map tool (headings, sections, tables)
- [ ] Implement AST-like navigation for documents
- [ ] Add get_context_around tool for targeted reads
- [ ] Enable jump-to-section without full reads

### 2.2 Incremental Context Building
- [ ] Start with document outline/structure
- [ ] Progressively load sections as needed
- [ ] Track "hot" sections being actively edited
- [ ] Unload cold sections from context

### 2.3 Smart Search & Navigation
- [ ] Implement fuzzy_jump_to tool (go to heading/table)
- [ ] Add context-aware search (search within section)
- [ ] Create breadcrumb navigation (current location)
- [ ] Enable relative navigation (next/previous section)

**Expected savings: 40-50% for navigation tasks**

## PHASE 3: MEMORY & COMPRESSION
## ------------------------------

### 3.1 Fix & Enhance Memory System
- [ ] Re-enable SummarizeTail compressor
- [ ] Implement proper token counting (use tiktoken or API)
- [ ] Add sliding window with importance scoring
- [ ] Create task-specific memory schemas

### 3.2 Intelligent Summarization
- [ ] Use Claude to summarize completed subtasks
- [ ] Preserve critical anchors (IDs, decisions, TODOs)
- [ ] Implement "memory checkpoint" system
- [ ] Add context resurrection from summaries

### 3.3 Working Memory Enhancement
- [ ] Expand schema: add document_state, edit_history
- [ ] Track document regions touched
- [ ] Maintain edit intention graph
- [ ] Store successful tool patterns

**Expected savings: 50-60% on long conversations**

## PHASE 4: ADAPTIVE TOOL RESPONSES
## ---------------------------------

### 4.1 Context-Aware Tool Behavior
- [ ] Tools detect available context and adapt
- [ ] Automatic response truncation near limits
- [ ] Progressive detail disclosure
- [ ] Smart pagination for large results

### 4.2 Tool Chaining Optimization
- [ ] Batch similar operations
- [ ] Pipeline tool calls to reduce round-trips
- [ ] Implement macro tools for common workflows
- [ ] Cache intermediate results

### 4.3 Failure Recovery
- [ ] Graceful degradation when hitting limits
- [ ] Automatic context pruning suggestions
- [ ] Checkpoint and resume for long tasks
- [ ] Memory overflow handling

**Expected savings: 20-30% on complex workflows**

## PHASE 5: ADVANCED OPTIMIZATIONS
## --------------------------------

### 5.1 Document Diffing System
- [ ] Implement efficient diff representation
- [ ] Track only changes between operations
- [ ] Use patch-based updates
- [ ] Compress unchanged content references

### 5.2 Semantic Context Selection
- [ ] Use embeddings for relevant section retrieval
- [ ] Implement importance scoring for content
- [ ] Smart context window packing
- [ ] Predictive prefetching based on task

### 5.3 Meta-Learning
- [ ] Track tool usage patterns
- [ ] Learn optimal context for task types
- [ ] Auto-generate macro tools from patterns
- [ ] Personalize based on user workflows

**Expected savings: Additional 20-30%**

## IMPLEMENTATION PRIORITIES
## -------------------------

### Quick Wins (1-2 days)
1. Add verbosity control to tools
2. Implement document_map tool
3. Fix token counting
4. Add response limiting

### Medium Term (3-5 days)
1. Tool consolidation
2. Re-enable memory compression
3. Smart navigation tools
4. Batch operations

### Long Term (1-2 weeks)
1. Full memory system overhaul
2. Semantic context selection
3. Advanced diffing
4. Meta-learning system

## SUCCESS METRICS
## ----------------
- Average tokens per operation: -50%
- Context resets per session: -75%
- Task completion rate: +20%
- Operations before limit: 3x increase

## TECHNICAL CONSTRAINTS
## ----------------------
- Must work within Mastra's agent/memory APIs
- Cannot modify core Mastra packages
- Frontend tools via SSE bridge only
- Maintain backward compatibility

## RISKS & MITIGATIONS
## --------------------
- **Risk**: Over-compression loses critical context
  **Mitigation**: Always preserve task goals and recent edits

- **Risk**: Tool consolidation increases complexity
  **Mitigation**: Maintain simple tool variants

- **Risk**: Smart selection misses important content
  **Mitigation**: Fallback to full reads when uncertain

## NOTES
## -----
- Start with Phase 1 for immediate impact
- Phases 2-3 are highest value for complex documents
- Phase 4-5 require more research and testing
- Consider A/B testing different strategies
- Monitor actual token usage vs estimates

## REFERENCES
## -----------
- Anthropic: Focus on high-impact workflows, response formats
- Cline: AST navigation, narrative integrity, passive memory
- Current implementation: /backend/src/mastra/
- Token limits: 160k input, 40k output (configurable)

Last updated: 2025-01-13
Status: Ready for implementation